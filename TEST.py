import os
import torch
import time
from hallucination_validator import HallucinationValidator

from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

from langchain_google_genai import ChatGoogleGenerativeAI

# ==========================================
# 1. AYARLAR
# ==========================================

# --- SEÃ‡Ä°M YAPIN ---
# "ollama" veya "gemini" yazarak motoru deÄŸiÅŸtirin.
LLM_PROVIDER = "ollama"  # veya "gemini"

# Qdrant ve Embedding AyarlarÄ±
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "Tubitak_Dokumanlar"
EMBEDDING_MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"

# Model Ä°simleri
OLLAMA_MODEL_NAME = "gemma3:12b"
GEMINI_MODEL_NAME = "gemini-2.5-flash"  # veya "gemini-1.5-pro"

# Cihaz SeÃ§imi (CUDA/CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Ã‡alÄ±ÅŸma Modu: {device.upper()}")

# ==========================================
# 2. BAÄLANTILAR (Client, Embedding)
# ==========================================

# Qdrant Client
try:
    client = QdrantClient(url=QDRANT_URL)
    client.get_collections()
    print("âœ… Qdrant baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±.")
except Exception as e:
    print(f"âŒ Qdrant'a baÄŸlanÄ±lamadÄ±: {e}")
    exit()

# Embedding Modeli (Her iki LLM iÃ§in de ortak)
print("ğŸ§  Embedding modeli yÃ¼kleniyor...")
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True}
)

# ==========================================
# 3. LLM KURULUMU (Ollama vs Gemini)
# ==========================================

llm = None

if LLM_PROVIDER == "gemini":
    print(f"ğŸ¤– LLM Modu: GOOGLE GEMINI ({GEMINI_MODEL_NAME}) hazÄ±rlanÄ±yor...")

    os.environ["GOOGLE_API_KEY"] = ""

    llm = ChatGoogleGenerativeAI(
        model=GEMINI_MODEL_NAME,
        temperature=0.1,
        max_retries=2,
    )

elif LLM_PROVIDER == "ollama":
    print(f"ğŸ¤– LLM Modu: LOCAL OLLAMA ({OLLAMA_MODEL_NAME}) hazÄ±rlanÄ±yor...")
    llm = OllamaLLM(
        model=OLLAMA_MODEL_NAME,
        temperature=0.1,
        top_p=0.9,
        repeat_penalty=1.1,
        num_predict=1024,
    )

else:
    raise ValueError("GeÃ§ersiz LLM_PROVIDER seÃ§imi! 'ollama' veya 'gemini' olmalÄ±.")


# ==========================================
# 4. YARDIMCI FONKSÄ°YONLAR (RAG)
# ==========================================

def get_vector_store():
    return QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
    )


def get_context_and_print(query: str, permission: str, doc_type: str = None, k: int = 3, SCORE_THRESHOLD=0.70):
    vector_store = get_vector_store()

    # Filtreler
    search_kwargs = {"k": k}
    must_conditions = []

    must_conditions.append(
        models.FieldCondition(
            key="metadata.permission",
            match=models.MatchValue(value=permission)
        )
    )

    if doc_type:
        must_conditions.append(
            models.FieldCondition(
                key="metadata.file_type",
                match=models.MatchValue(value=doc_type)
            )
        )

    if must_conditions:
        search_kwargs["filter"] = models.Filter(must=must_conditions)

    # Arama Yap
    docs_with_scores = vector_store.similarity_search_with_score(
        query,
        **search_kwargs
    )

    context_parts = []

    print("\n" + "=" * 50)
    print("ğŸ” VEKTÃ–R SONUÃ‡LARI ANALÄ°ZÄ°")
    print("=" * 50)

    filtered_docs = []
    for doc, score in docs_with_scores:
        if score >= SCORE_THRESHOLD:
            filtered_docs.append((doc, score))
        else:
            print(f"âš ï¸ ELENDÄ° (DÃ¼ÅŸÃ¼k Skor: {score:.4f}) - {doc.metadata.get('source')}")

    if not filtered_docs:
        print("âŒ Yeterince benzer sonuÃ§ bulunamadÄ± (EÅŸik altÄ±).")
        return None, []

    docs_list = []
    for i, (doc, score) in enumerate(filtered_docs, 1):
        doc.metadata["score"] = score
        docs_list.append(doc)
        print(f"\nğŸ“„ [CHUNK {i}] (Benzerlik Skoru: {score:.4f})")
        print(f"   ğŸ“‚ Kaynak: {doc.metadata.get('source')}")
        print(f"   ğŸ”’ Yetki: {doc.metadata.get('permission')}")
        print("-" * 30)
        print(f"{doc.page_content}")
        print("-" * 30)
        context_parts.append(doc.page_content)

    return "\n\n---\n\n".join(context_parts), docs_list


# ==========================================
# 5. ANA Ã‡ALIÅTIRMA FONKSÄ°YONU
# ==========================================
import time
def run_rag_pipeline(question: str, permission: str, doc_type: str = None, k: int = 3, SCORE_THRESHOLD=0.70):
    print(f"\nğŸ“¥ KULLANICI SORUSU: {question}")

    # 1. ChunklarÄ± getir BURASI
    baslangic = time.perf_counter()
    context_text, retrieved_docs = get_context_and_print(question, permission, doc_type, k, SCORE_THRESHOLD)

    bitis = time.perf_counter()
    gecen_sure_ms = (bitis - baslangic) * 1000

    print(f"Ä°ÅŸlem sÃ¼resi: {gecen_sure_ms:.2f} ms")

    if not context_text:
        print("\n" + "=" * 50)
        print("ğŸ¤– SÄ°STEM CEVABI")
        print("=" * 50)
        print("\nBu konuda belgelerimde yeterli kalitede bilgi bulunamadÄ±.\n")
        print("=" * 50)
        return

    # 2. Prompt HazÄ±rla
    prompt_template = """Sen yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.
    EÄŸer baÄŸlamda cevabÄ± bulamazsan, uydurma, sadece "Bilgim yok" de.

    BaÄŸlam (VeritabanÄ±ndan Gelen Bilgi):
    {context}

    KullanÄ±cÄ± Sorusu:
    {question}

    KATÃ KURALLAR:
    1. SADECE yukarÄ±daki baÄŸlamdan cevap ver.
    2. BaÄŸlamda cevap YOKSA: "Bu konuda belgelerimde bilgi bulunmuyor" de.
    3. ASLA tahmin yapma veya kendi bilgini kullanma.
    4. Belirsizlik varsa aÃ§Ä±kÃ§a belirt.

    Cevap:"""

    final_prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = final_prompt | llm

    # 3. Modele GÃ¶nder
    print("\n" + "=" * 50)
    print(f"ğŸ¤– {LLM_PROVIDER.upper()} MODELÄ° DÃœÅÃœNÃœYOR...")
    print("=" * 50)

    response = chain.invoke({
        "context": context_text,
        "question": question
    })

    # 4. CevabÄ± YazdÄ±r
    # Gemini bazen "content" objesi dÃ¶nebilir, Ollama string dÃ¶ner. LangChain bunu genelde yÃ¶netir ama
    # garanti olsun diye string'e Ã§evirelim veya doÄŸrudan yazdÄ±ralÄ±m.

    final_response = response.content if hasattr(response, 'content') else response

    is_valid, validation_msg = HallucinationValidator.validate_response(
        question, final_response, retrieved_docs
    )
    
    if not is_valid:
        print(f"\nâš ï¸ HALÃœSINASYON UYARISI: {validation_msg}")
        print("Sistem gÃ¼venlik nedeniyle yanÄ±tÄ± reddetdi.\n")
        print("=" * 50)
        return

    print(f"\n{final_response}\n")
    print("=" * 50)


# ==========================================
# 6. TEST ALANI
# ==========================================

if __name__ == "__main__":

    # SayacÄ± baÅŸlat

    soru = "BAP komisyonu kimlerden oluÅŸur?"

    run_rag_pipeline(
        soru,
        permission="manager",
        doc_type="pdf",
        k=5,
        SCORE_THRESHOLD=0.70
    )