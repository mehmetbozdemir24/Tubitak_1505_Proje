import os
import torch

# LangChain ve Qdrant KÃ¼tÃ¼phaneleri
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# ==========================================
# 1. AYARLAR
# ==========================================
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "Tubitak_Dokumanlar"
EMBEDDING_MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"
LLM_MODEL_NAME = "gemma3:12b"   #gemma3:12b qwen2.5:14b

# Cihaz SeÃ§imi (CUDA/CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Ã‡alÄ±ÅŸma Modu: {device.upper()}")

# ==========================================
# 2. BAÄLANTILAR (Client, Embedding, LLM)
# ==========================================

# Qdrant Client
try:
    client = QdrantClient(url=QDRANT_URL)
    # BaÄŸlantÄ±yÄ± test et
    client.get_collections()
    print("âœ… Qdrant baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±.")
except Exception as e:
    print(f"âŒ Qdrant'a baÄŸlanÄ±lamadÄ±: {e}")
    print("Docker konteynerinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.")
    exit()

# Embedding Modeli
print("ğŸ§  Embedding modeli yÃ¼kleniyor...")
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True}
)

# LLM (Gemma 3)
print("ğŸ¤– LLM (Gemma) hazÄ±rlanÄ±yor...")
llm = OllamaLLM(
    model=LLM_MODEL_NAME,
    temperature=0.1,
    top_p=0.9,
    repeat_penalty=1.1,
    num_predict=1024,
)


# ==========================================
# 3. YARDIMCI FONKSÄ°YONLAR
# ==========================================

def get_vector_store():
    """LangChain uyumlu VectorStore nesnesini dÃ¶ndÃ¼rÃ¼r"""
    return QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
    )


def get_context_and_print(query: str, doc_type: str = None):
    """
    VeritabanÄ±ndan bilgiyi Ã§eker ve EKRANA YAZDIRIR.
    """
    vector_store = get_vector_store()

    # Filtreleme MantÄ±ÄŸÄ±
    search_kwargs = {"k": 3}  # En alakalÄ± 3 parÃ§ayÄ± getir

    if doc_type:
        print(f"â„¹ï¸ Filtre uygulanÄ±yor: {doc_type}")
        filter_condition = models.Filter(
            must=[
                models.FieldCondition(
                    key="metadata.file_type",
                    match=models.MatchValue(value=doc_type)
                )
            ]
        )
        search_kwargs["filter"] = filter_condition

    # Semantik Arama Yap
    docs_with_scores = vector_store.similarity_search_with_score(
        query,
        **search_kwargs
    )

    # Gelen iÃ§erikleri birleÅŸtir ve YAZDIR
    context_parts = []

    print("\n" + "=" * 50)
    print("ğŸ” VEKTÃ–R VERÄ°TABANINDAN GETÄ°RÄ°LEN CHUNK'LAR")
    print("=" * 50)

    if not docs_with_scores:
        print("âŒ HiÃ§bir eÅŸleÅŸme bulunamadÄ±!")
        return None

    for i, (doc, score) in enumerate(docs_with_scores, 1):
        source = doc.metadata.get("source", "Bilinmiyor")
        sheet = doc.metadata.get("sheet", "-")

        print(f"\nğŸ“„ [CHUNK {i}] (Benzerlik Skoru: {score:.4f})")
        print(f"   ğŸ“‚ Kaynak: {source}")
        if sheet != "-": print(f"   ğŸ“‘ Sayfa: {sheet}")
        print("-" * 30)
        print(f"{doc.page_content}")
        print("-" * 30)

        context_parts.append(doc.page_content)

    return "\n\n---\n\n".join(context_parts)


# ==========================================
# 4. ANA Ã‡ALIÅTIRMA FONKSÄ°YONU
# ==========================================

def run_rag_pipeline(question: str, doc_type: str = None):
    print(f"\nğŸ“¥ KULLANICI SORUSU: {question}")

    # 1. ChunklarÄ± getir ve yazdÄ±r
    context_text = get_context_and_print(question, doc_type)

    if not context_text:
        print("âš ï¸ Yeterli bilgi bulunamadÄ±ÄŸÄ± iÃ§in model Ã§alÄ±ÅŸtÄ±rÄ±lmadÄ±.")
        return

    # 2. Prompt HazÄ±rla
    prompt_template = """Sen yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.
    EÄŸer baÄŸlamda cevabÄ± bulamazsan, uydurma, sadece "Bilgim yok" de.

    BaÄŸlam (VeritabanÄ±ndan Gelen Bilgi):
    {context}

    KullanÄ±cÄ± Sorusu:
    {question}

    Cevap:"""

    final_prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = final_prompt | llm

    # 3. Modele GÃ¶nder
    print("\n" + "=" * 50)
    print("ğŸ¤– GEMMA MODELÄ° DÃœÅÃœNÃœYOR...")
    print("=" * 50)

    response = chain.invoke({
        "context": context_text,
        "question": question
    })

    # 4. CevabÄ± YazdÄ±r
    print(f"\n{response}\n")
    print("=" * 50)


# ==========================================
# 5. TEST ALANI (BurayÄ± DeÄŸiÅŸtirip Ã‡alÄ±ÅŸtÄ±r)
# ==========================================
if __name__ == "__main__":
    # BURAYA Ä°STEDÄ°ÄÄ°N SORUYU YAZ
    soru = "Disiplin kurulu Ã¼yeleri kimlerden oluÅŸur ve kim tarafÄ±ndan seÃ§ilir?"

    # doc_type="excel" diyerek sadece excelde aratabilirsin,
    # veya None diyerek hepsinde aratabilirsin.

    run_rag_pipeline(soru, doc_type="pdf")