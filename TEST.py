import os
import torch

# LangChain ve Qdrant KÃ¼tÃ¼phaneleri
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# ==========================================
# 1. AYARLAR
# ==========================================
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "Tubitak_Dokumanlar"
EMBEDDING_MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"
LLM_MODEL_NAME = "gemma3:12b"

# Cihaz SeÃ§imi (CUDA/CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Ã‡alÄ±ÅŸma Modu: {device.upper()}")

# ==========================================
# 2. BAÄLANTILAR (Client, Embedding, LLM)
# ==========================================

try:
    client = QdrantClient(url=QDRANT_URL)
    client.get_collections()
    print("âœ… Qdrant baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±.")
except Exception as e:
    print(f"âŒ Qdrant'a baÄŸlanÄ±lamadÄ±: {e}")
    exit()

print("ğŸ§  Embedding modeli yÃ¼kleniyor...")
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True}
)

print("ğŸ¤– LLM (Gemma) hazÄ±rlanÄ±yor...")
llm = OllamaLLM(
    model=LLM_MODEL_NAME,
    temperature=0.1,
    top_p=0.9,
    repeat_penalty=1.1,
    num_predict=1024,
)


# ==========================================
# 3. YARDIMCI FONKSÄ°YONLAR
# ==========================================

def get_vector_store():
    return QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
    )

# GÃœNCELLEME 1: k parametresi eklendi (varsayÄ±lan 3)
def get_context_and_print(query: str, permission: str, doc_type: str = None, k: int = 3):
    vector_store = get_vector_store()

    # 1. FÄ°LTRELERÄ° HAZIRLA
    # GÃœNCELLEME: k parametresi buraya baÄŸlandÄ±
    search_kwargs = {"k": k}
    must_conditions = []

    must_conditions.append(
        models.FieldCondition(
            key="metadata.permission",
            match=models.MatchValue(value=permission)
        )
    )

    if doc_type:
        must_conditions.append(
            models.FieldCondition(
                key="metadata.file_type",
                match=models.MatchValue(value=doc_type)
            )
        )

    if must_conditions:
        search_kwargs["filter"] = models.Filter(must=must_conditions)

    docs_with_scores = vector_store.similarity_search_with_score(
        query,
        **search_kwargs
    )

    context_parts = []

    # EÅŸik DeÄŸeri
    SCORE_THRESHOLD = 0.60

    print("\n" + "=" * 50)
    print("ğŸ” VEKTÃ–R SONUÃ‡LARI ANALÄ°ZÄ°")
    print("=" * 50)

    filtered_docs = []
    for doc, score in docs_with_scores:
        if score >= SCORE_THRESHOLD:
            filtered_docs.append((doc, score))
        else:
            print(f"âš ï¸ ELENDÄ° (DÃ¼ÅŸÃ¼k Skor: {score:.4f}) - {doc.metadata.get('source')}")

    # EÄŸer hiÃ§ belge kalmadÄ±ysa None dÃ¶nÃ¼yoruz
    if not filtered_docs:
        print("âŒ Yeterince benzer sonuÃ§ bulunamadÄ± (EÅŸik altÄ±).")
        return None

    for i, (doc, score) in enumerate(filtered_docs, 1):
        print(f"\nğŸ“„ [CHUNK {i}] (Benzerlik Skoru: {score:.4f})")
        print(f"   ğŸ“‚ Kaynak: {doc.metadata.get('source')}")
        print(f"   ğŸ”’ Yetki: {doc.metadata.get('permission')}")
        print("-" * 30)
        print(f"{doc.page_content}")
        print("-" * 30)
        context_parts.append(doc.page_content)

    return "\n\n---\n\n".join(context_parts)


# ==========================================
# 4. ANA Ã‡ALIÅTIRMA FONKSÄ°YONU
# ==========================================

# GÃœNCELLEME 2: k parametresi buraya da eklendi
def run_rag_pipeline(question: str, permission: str, doc_type: str = None, k: int = 3):
    print(f"\nğŸ“¥ KULLANICI SORUSU: {question}")

    # 1. ChunklarÄ± getir
    context_text = get_context_and_print(question, permission, doc_type, k)

    # GÃœNCELLEME 3: EÄŸer context yoksa (eÅŸik altÄ±ndaysa), doÄŸrudan "Bilgim yok" de.
    if not context_text:
        print("\n" + "=" * 50)
        print("ğŸ¤– SÄ°STEM CEVABI (Model Ã‡alÄ±ÅŸtÄ±rÄ±lmadÄ±)")
        print("=" * 50)
        print("\nBilgim yok.\n") # KullanÄ±cÄ±nÄ±n gÃ¶receÄŸi cevap
        print("=" * 50)
        return

    # 2. Prompt HazÄ±rla
    prompt_template = """Sen yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.
    EÄŸer baÄŸlamda cevabÄ± bulamazsan, uydurma, sadece "Bilgim yok" de.

    BaÄŸlam (VeritabanÄ±ndan Gelen Bilgi):
    {context}

    KullanÄ±cÄ± Sorusu:
    {question}

    Cevap:"""

    final_prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = final_prompt | llm

    # 3. Modele GÃ¶nder
    print("\n" + "=" * 50)
    print("ğŸ¤– GEMMA MODELÄ° DÃœÅÃœNÃœYOR...")
    print("=" * 50)

    response = chain.invoke({
        "context": context_text,
        "question": question
    })

    # 4. CevabÄ± YazdÄ±r
    print(f"\n{response}\n")
    print("=" * 50)


# ==========================================
# 5. TEST ALANI
# ==========================================
if __name__ == "__main__":

    #soru = "Disiplin kurulu Ã¼yeleri kimlerden oluÅŸur ve kim tarafÄ±ndan seÃ§ilir?"
    soru = "Disiplin  suÃ§ ve cezalarÄ± nelerdir?"

    run_rag_pipeline(soru, permission="user", doc_type="pdf", k=5)