import os
import torch
import getpass

# LangChain ve Qdrant KÃ¼tÃ¼phaneleri
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient, models
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# Google Gemini KÃ¼tÃ¼phanesi
from langchain_google_genai import ChatGoogleGenerativeAI

# ==========================================
# 1. AYARLAR
# ==========================================

# --- SEÃ‡Ä°M YAPIN ---
# "ollama" veya "gemini" yazarak motoru deÄŸiÅŸtirin.
LLM_PROVIDER = "ollama"

# Qdrant ve Embedding AyarlarÄ±
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "Tubitak_Dokumanlar"
EMBEDDING_MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"

# Model Ä°simleri
OLLAMA_MODEL_NAME = "gemma3:12b"
GEMINI_MODEL_NAME = "gemini-2.5-flash"  # veya "gemini-1.5-pro"

# Cihaz SeÃ§imi (CUDA/CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âš™ï¸ Ã‡alÄ±ÅŸma Modu: {device.upper()}")

# ==========================================
# 2. BAÄLANTILAR (Client, Embedding)
# ==========================================

# Qdrant Client
try:
    client = QdrantClient(url=QDRANT_URL)
    client.get_collections()
    print("âœ… Qdrant baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±.")
except Exception as e:
    print(f"âŒ Qdrant'a baÄŸlanÄ±lamadÄ±: {e}")
    exit()

# Embedding Modeli (Her iki LLM iÃ§in de ortak)
print("ğŸ§  Embedding modeli yÃ¼kleniyor...")
embeddings = HuggingFaceEmbeddings(
    model_name=EMBEDDING_MODEL_NAME,
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True}
)

# ==========================================
# 3. LLM KURULUMU (Ollama vs Gemini)
# ==========================================

llm = None

if LLM_PROVIDER == "gemini":
    print(f"ğŸ¤– LLM Modu: GOOGLE GEMINI ({GEMINI_MODEL_NAME}) hazÄ±rlanÄ±yor...")

    os.environ["GOOGLE_API_KEY"] = ""

    llm = ChatGoogleGenerativeAI(
        model=GEMINI_MODEL_NAME,
        temperature=0.1,
        max_retries=2,
    )

elif LLM_PROVIDER == "ollama":
    print(f"ğŸ¤– LLM Modu: LOCAL OLLAMA ({OLLAMA_MODEL_NAME}) hazÄ±rlanÄ±yor...")
    llm = OllamaLLM(
        model=OLLAMA_MODEL_NAME,
        temperature=0.1,
        top_p=0.9,
        repeat_penalty=1.1,
        num_predict=1024,
    )

else:
    raise ValueError("GeÃ§ersiz LLM_PROVIDER seÃ§imi! 'ollama' veya 'gemini' olmalÄ±.")


# ==========================================
# 4. YARDIMCI FONKSÄ°YONLAR (RAG)
# ==========================================

def get_vector_store():
    return QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        embedding=embeddings,
    )


def get_context_and_print(query: str, permission: str, doc_type: str = None, k: int = 3, SCORE_THRESHOLD=0.50):
    vector_store = get_vector_store()

    # Filtreler
    search_kwargs = {"k": k}
    must_conditions = []

    must_conditions.append(
        models.FieldCondition(
            key="metadata.permission",
            match=models.MatchValue(value=permission)
        )
    )

    if doc_type:
        must_conditions.append(
            models.FieldCondition(
                key="metadata.file_type",
                match=models.MatchValue(value=doc_type)
            )
        )

    if must_conditions:
        search_kwargs["filter"] = models.Filter(must=must_conditions)

    # Arama Yap
    docs_with_scores = vector_store.similarity_search_with_score(
        query,
        **search_kwargs
    )

    context_parts = []

    print("\n" + "=" * 50)
    print("ğŸ” VEKTÃ–R SONUÃ‡LARI ANALÄ°ZÄ°")
    print("=" * 50)

    filtered_docs = []
    for doc, score in docs_with_scores:
        if score >= SCORE_THRESHOLD:
            filtered_docs.append((doc, score))
        else:
            print(f"âš ï¸ ELENDÄ° (DÃ¼ÅŸÃ¼k Skor: {score:.4f}) - {doc.metadata.get('source')}")

    if not filtered_docs:
        print("âŒ Yeterince benzer sonuÃ§ bulunamadÄ± (EÅŸik altÄ±).")
        return None

    for i, (doc, score) in enumerate(filtered_docs, 1):
        print(f"\nğŸ“„ [CHUNK {i}] (Benzerlik Skoru: {score:.4f})")
        print(f"   ğŸ“‚ Kaynak: {doc.metadata.get('source')}")
        print(f"   ğŸ”’ Yetki: {doc.metadata.get('permission')}")
        print("-" * 30)
        print(f"{doc.page_content}")
        print("-" * 30)
        context_parts.append(doc.page_content)

    return "\n\n---\n\n".join(context_parts)


# ==========================================
# 5. ANA Ã‡ALIÅTIRMA FONKSÄ°YONU
# ==========================================

def run_rag_pipeline(question: str, permission: str, doc_type: str = None, k: int = 3, SCORE_THRESHOLD=0.50):
    print(f"\nğŸ“¥ KULLANICI SORUSU: {question}")

    # 1. ChunklarÄ± getir
    context_text = get_context_and_print(question, permission, doc_type, k, SCORE_THRESHOLD)

    # Context yoksa iptal et
    if not context_text:
        print("\n" + "=" * 50)
        print("ğŸ¤– SÄ°STEM CEVABI")
        print("=" * 50)
        print("\nBilgim yok.\n")
        print("=" * 50)
        return

    # 2. Prompt HazÄ±rla
    prompt_template = """Sen yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n. AÅŸaÄŸÄ±daki baÄŸlam bilgisini kullanarak kullanÄ±cÄ±nÄ±n sorusunu cevapla.
    EÄŸer baÄŸlamda cevabÄ± bulamazsan, uydurma, sadece "Bilgim yok" de.

    BaÄŸlam (VeritabanÄ±ndan Gelen Bilgi):
    {context}

    KullanÄ±cÄ± Sorusu:
    {question}

    Cevap:"""

    final_prompt = ChatPromptTemplate.from_template(prompt_template)
    chain = final_prompt | llm

    # 3. Modele GÃ¶nder
    print("\n" + "=" * 50)
    print(f"ğŸ¤– {LLM_PROVIDER.upper()} MODELÄ° DÃœÅÃœNÃœYOR...")
    print("=" * 50)

    response = chain.invoke({
        "context": context_text,
        "question": question
    })

    # 4. CevabÄ± YazdÄ±r
    # Gemini bazen "content" objesi dÃ¶nebilir, Ollama string dÃ¶ner. LangChain bunu genelde yÃ¶netir ama
    # garanti olsun diye string'e Ã§evirelim veya doÄŸrudan yazdÄ±ralÄ±m.

    final_response = response.content if hasattr(response, 'content') else response

    print(f"\n{final_response}\n")
    print("=" * 50)


# ==========================================
# 6. TEST ALANI
# ==========================================
if __name__ == "__main__":
    soru = "Haziran ayÄ±nda hedef cirosu en fazla olan ilk 2 Ã¼rÃ¼nÃ¼nÃ¼n toplam hedef cirosu nedir?"

    run_rag_pipeline(
        soru,
        permission="user",
        doc_type="excel",
        k=5,
        SCORE_THRESHOLD=0.45
    )


#%%
import os
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings

# 1. MODELÄ° YÃœKLE
MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"
embeddings = HuggingFaceEmbeddings(
    model_name=MODEL_NAME,
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True}
)

# 2. METÄ°N
text = """
Futbol dÃ¼nyada en Ã§ok izlenen spor dalÄ±dÄ±r. 
MaÃ§lar 90 dakika sÃ¼rer ve iki takÄ±mÄ±n mÃ¼cadelesine sahne olur.
Ofsayt kuralÄ±, oyunun en tartÄ±ÅŸmalÄ± kurallarÄ±ndan biridir.
Son DÃ¼nya KupasÄ± finali milyonlarca kiÅŸi tarafÄ±ndan izlendi.
Hakem kararlarÄ± maÃ§Ä±n kaderini deÄŸiÅŸtirebilir.

Åimdi biraz da mutfaÄŸa girelim ve gÃ¼zel bir kek yapalÄ±m.
Ã–nce yumurta ve ÅŸekeri kÃ¶pÃ¼rene kadar Ã§Ä±rpÄ±n.
ArdÄ±ndan un, kabartma tozu ve vanilyayÄ± ekleyip karÄ±ÅŸtÄ±rÄ±n.
KarÄ±ÅŸÄ±mÄ± yaÄŸlanmÄ±ÅŸ kalÄ±ba dÃ¶kÃ¼p 180 derece fÄ±rÄ±na verin.
KÃ¼rdan testi yaparak piÅŸip piÅŸmediÄŸini kontrol edebilirsiniz.
Ã‡ayÄ±n yanÄ±nda servis yapmanÄ±zÄ± Ã¶neririm.

OsmanlÄ± Ä°mparatorluÄŸu 1299 yÄ±lÄ±nda SÃ¶ÄŸÃ¼t'te kurulmuÅŸtur.
Fatih Sultan Mehmet 1453 yÄ±lÄ±nda Ä°stanbul'u fethederek Ã§aÄŸ aÃ§Ä±p Ã§aÄŸ kapatmÄ±ÅŸtÄ±r.
Ä°mparatorluk Ã¼Ã§ kÄ±taya yayÄ±larak geniÅŸ bir coÄŸrafyaya hÃ¼kmetmiÅŸtir.
Yavuz Sultan Selim dÃ¶neminde hazine tam doluluÄŸa ulaÅŸmÄ±ÅŸtÄ±r.
TarihÃ§iler bu dÃ¶nemi yÃ¼kselme dÃ¶nemi olarak adlandÄ±rÄ±r.
"""

# 3. SEMANTIC CHUNKER AYARLARI (DÃœZELTÄ°LDÄ°)
# 'percentile' kullanÄ±yoruz ve 95 yapÄ±yoruz.
# Yani: "Anlamsal farkÄ±n en yÃ¼ksek olduÄŸu %5'lik noktalardan bÃ¶l."
text_splitter = SemanticChunker(
    embeddings,
    breakpoint_threshold_type="percentile",
    breakpoint_threshold_amount=80
)

# 4. PARÃ‡ALA
print("ğŸ”ª Metin anlamsal olarak parÃ§alanÄ±yor...")
docs = text_splitter.create_documents([text])

# BoÅŸ chunklarÄ± temizleme filtresi
clean_docs = [d for d in docs if d.page_content.strip() != ""]

print(f"\nâœ… SONUÃ‡: Metin toplam {len(clean_docs)} dolu parÃ§aya bÃ¶lÃ¼ndÃ¼.\n")

for i, doc in enumerate(clean_docs, 1):
    print(f"--- ğŸ“„ CHUNK {i} ---")
    print(doc.page_content)
    print("-" * 30)