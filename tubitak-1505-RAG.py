import os
import time
import json
import hashlib
import tempfile
import requests
import torch
import re
from uuid import uuid4

import streamlit as st
from streamlit_lottie import st_lottie

from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.tools import tool
from hallucination_validator import HallucinationValidator

# --- SAYFA AYARLARI ---
st.set_page_config(page_title="Bilimp AI Asistan", layout="wide", page_icon="ğŸ¤–")


# --- ANÄ°MASYON YÃœKLEME ---
def load_lottieurl(url: str):
    try:
        r = requests.get(url)
        if r.status_code != 200: return None
        return r.json()
    except:
        return None


# --- YARDIMCI FONKSÄ°YON: METÄ°N AKIÅI SÄ°MÃœLASYONU ---
# Tool kullanÄ±lmadÄ±ÄŸÄ±nda hazÄ±r olan metni akÄ±ÅŸkan gÃ¶stermek iÃ§in
def stream_text_generator(text):
    for word in text.split(" "):
        yield word + " "
        time.sleep(0.05)


# --- GÃ–RSEL YÃœKLEME EKRANI ---
if "app_loaded" not in st.session_state:
    loader_placeholder = st.empty()
    with loader_placeholder.container():
        st.markdown(
            """<style>.stApp {background-color: #0e1117;} .glowing-text {font-family: 'Source Code Pro', monospace; color: #00fbff; text-align: center; font-size: 2em; font-weight: bold; text-shadow: 0 0 10px #00fbff; animation: pulse 1.5s infinite;} @keyframes pulse { from {opacity: 0.8;} to {opacity: 1;} }</style>""",
            unsafe_allow_html=True)
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            lottie_json = load_lottieurl("https://lottie.host/file/9860f43c-6232-4665-ba4f-557c669299b6.json")
            if lottie_json: st_lottie(lottie_json, height=250, key="loader", speed=1.5)

        status_text_placeholder = st.empty()
        loading_steps = ["ğŸ§  NÃ¶ral AÄŸlar YÃ¼kleniyor...", "âš¡ GPU HÄ±zlandÄ±rma Aktif...",
                         "ğŸ› ï¸ Streaming (AkÄ±ÅŸ) ModÃ¼lÃ¼ BaÅŸlatÄ±lÄ±yor...", "ğŸš€ LÃ¼tfen Bekleyiniz Sistem HazÄ±rlanÄ±yor..."]
        for step in loading_steps:
            status_text_placeholder.markdown(f'<p class="glowing-text">{step}</p>', unsafe_allow_html=True)
            time.sleep(0.5)

        # --- IMPORTLAR ---
        from qdrant_client import QdrantClient
        from qdrant_client.http import models as rest_models
        from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams, Filter, FieldCondition, \
            MatchValue, MatchAny
        from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
        from langchain_huggingface import HuggingFaceEmbeddings
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain_ollama import ChatOllama
        from langchain_core.output_parsers import StrOutputParser
        import pymupdf4llm
        from markitdown import MarkItDown
        from pptx import Presentation
        from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
        from langchain_core.documents import Document

    loader_placeholder.empty()
    st.session_state["app_loaded"] = True
else:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models as rest_models
    from qdrant_client.http.models import Distance, VectorParams, SparseVectorParams, Filter, FieldCondition, \
        MatchValue, MatchAny
    from langchain_qdrant import QdrantVectorStore, FastEmbedSparse, RetrievalMode
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_google_genai import ChatGoogleGenerativeAI
    from langchain_ollama import ChatOllama
    from langchain_core.output_parsers import StrOutputParser
    import pymupdf4llm
    from markitdown import MarkItDown
    from pptx import Presentation
    from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
    from langchain_core.documents import Document

# ==============================================================================
# AYARLAR
# ==============================================================================
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "Tubitak_Dokumanlar_Hybrid"
EMBEDDING_MODEL_NAME = "ytu-ce-cosmos/turkish-e5-large"
REGISTRY_FILE = "belge_kayitlari.json"


# ==============================================================================
# YARDIMCI FONKSÄ°YONLAR
# ==============================================================================
def calculate_md5(file_bytes):
    hash_md5 = hashlib.md5()
    hash_md5.update(file_bytes)
    return hash_md5.hexdigest()


def load_registry():
    if os.path.exists(REGISTRY_FILE):
        with open(REGISTRY_FILE, "r", encoding="utf-8") as f: return json.load(f)
    return {}


def save_registry(registry):
    with open(REGISTRY_FILE, "w", encoding="utf-8") as f: json.dump(registry, f, ensure_ascii=False, indent=4)


def delete_document_globally(filename):
    delete_by_source(filename)
    reg = load_registry()
    if filename in reg:
        del reg[filename]
        save_registry(reg)


def get_allowed_permissions(role):
    hierarchy = {"public": ["public"], "user": ["public", "user"], "management": ["public", "user", "management"],
                 "admin": ["public", "user", "management", "admin", "private"], "private": ["private"]}
    return hierarchy.get(role, ["public"])


def get_local_ollama_models():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=1)
        if response.status_code == 200: return [m["name"] for m in response.json().get("models", [])]
    except:
        return []
    return []


# ==============================================================================
# CHUNKLAMA VE PARSE Ä°ÅLEMLERÄ°
# ==============================================================================
def etiketleri_generic_duzelt(text):
    lines = text.split('\n')
    new_lines = []
    for line in lines:
        stripped = line.strip()
        if stripped.startswith("|") or (stripped.startswith("**") and stripped.endswith("**")):
            new_lines.append(line)
        else:
            new_lines.append(line)
    return '\n'.join(new_lines)


def process_pptx_native(file_path, source_name, permission):
    prs = Presentation(file_path)
    slides_chunks = []
    for i, slide in enumerate(prs.slides):
        content = []
        if slide.shapes.title and slide.shapes.title.text: content.append(f"# {slide.shapes.title.text.strip()}")
        for shape in slide.shapes:
            if hasattr(shape, "text_frame") and shape.text_frame: content.append(shape.text.strip())
        full = "\n\n".join(content)
        if full.strip():
            doc = Document(page_content=full, metadata={"source": source_name, "chunk_no": i + 1, "file_type": "pptx",
                                                        "permission": permission})
            slides_chunks.append(doc)
    return slides_chunks


def process_text_based(file_path, source_name, chunk_size, chunk_overlap, permission):
    ext = os.path.splitext(file_path)[1].lower()
    text = ""
    try:
        # 1. Markdown DÃ¶nÃ¼ÅŸÃ¼mÃ¼
        if ext == ".pdf":
            text = pymupdf4llm.to_markdown(file_path, write_images=False)
        else:
            md = MarkItDown()
            result = md.convert(file_path)
            text = result.text_content

        # Temizlik
        clean = etiketleri_generic_duzelt(text)

        # 2. BaÅŸlÄ±klara GÃ¶re BÃ¶lme
        headers_to_split_on = [
            ("#", "Main"),
            ("##", "Sub"),
            ("###", "Sub2"),
            ("####", "Sub3")
        ]

        splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            strip_headers=True
        )
        md_docs = splitter.split_text(clean)

        # --- AKILLI BÄ°RLEÅTÄ°RME 2.0 (SAFE MERGE) ---
        merged_docs = []
        temp_doc = None

        for doc in md_docs:
            if not doc.page_content.strip():
                continue

            # Context (BaÄŸlam) bilgisini hazÄ±rla
            header_path = " > ".join([doc.metadata.get(h[1]) for h in headers_to_split_on if doc.metadata.get(h[1])])
            if header_path:
                doc.page_content = f"**BAÄLAM:** {header_path}\n\n{doc.page_content}"

            # --- MANTIK BAÅLIYOR ---

            # EÄŸer elimizde bekleyen "yetim" bir parÃ§a varsa:
            if temp_doc:
                # GÃœVENLÄ°K KONTROLÃœ: Åimdiki parÃ§a da Ã§ok kÄ±saysa (baÅŸka bir baÅŸlÄ±k olabilir), birleÅŸtirme yapma!
                # Ã‡Ã¼nkÃ¼ "Yemek Listesi" baÅŸlÄ±ÄŸÄ± ile "Servis Saatleri" baÅŸlÄ±ÄŸÄ±nÄ± birleÅŸtirmek istemeyiz.
                if len(doc.page_content) < 100 and "|" not in doc.page_content:
                    # Bekleyeni olduÄŸu gibi kaydet, Ã§Ã¼nkÃ¼ arkasÄ±ndan gelen de iÃ§erik deÄŸilmiÅŸ.
                    merged_docs.append(temp_doc)
                    temp_doc = doc  # Åimdikini yeni bekleyen yap
                else:
                    # Åimdiki parÃ§a dolu bir iÃ§erik (Tablo veya Uzun Metin). BirleÅŸtir!
                    # Ã–nceki kÄ±sa baÅŸlÄ±k + Yeni SatÄ±r + Åimdiki Ä°Ã§erik
                    new_content = f"{temp_doc.page_content}\n\n{doc.page_content}"
                    doc.page_content = new_content
                    # Metadata'yÄ± koru (genelde aynÄ± baÅŸlÄ±k altÄ±ndadÄ±rlar)
                    merged_docs.append(doc)
                    temp_doc = None  # Bekleyen kutusunu boÅŸalt

            else:
                # Elimizde bekleyen yok. Peki bu parÃ§a beklemeye alÄ±nmalÄ± mÄ±?
                # Kural: 250 karakterden kÄ±saysa VE iÃ§inde Tablo yoksa -> Potansiyel Yetim BaÅŸlÄ±k
                if len(doc.page_content) < 250 and "|" not in doc.page_content:
                    temp_doc = doc
                else:
                    # ParÃ§a zaten bÃ¼yÃ¼k veya tablo, direkt ekle.
                    merged_docs.append(doc)

        # DÃ¶ngÃ¼ bittiÄŸinde elde kalan son parÃ§a varsa onu da ekle (Unutma!)
        if temp_doc:
            merged_docs.append(temp_doc)
        # -------------------------------------------------------------

        # 3. Recursive Splitter (Ã‡ok bÃ¼yÃ¼kleri bÃ¶lmek iÃ§in)
        rec_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", " ", ""]
        )

        final_docs = []
        for doc in merged_docs:
            doc.metadata.update({
                "source": source_name,
                "file_type": ext.replace(".", ""),
                "permission": permission
            })
            chunks = rec_splitter.split_documents([doc])
            final_docs.extend(chunks)

        return final_docs

    except Exception as e:
        st.error(f"Hata: {e}")
        return []


# ==============================================================================
# QDRANT VE EMBEDDING MODELLERÄ°
# ==============================================================================
@st.cache_resource
def get_dense_embeddings():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    return HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME, model_kwargs={"device": device},
                                 encode_kwargs={"normalize_embeddings": True})


@st.cache_resource
def get_sparse_embeddings():
    return FastEmbedSparse(model_name="Qdrant/bm25")


@st.cache_resource
def get_qdrant_client():
    return QdrantClient(url=QDRANT_URL, check_compatibility=False)


def init_collection():
    client = get_qdrant_client()
    if not client.collection_exists(COLLECTION_NAME):
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config={"content": VectorParams(size=1024, distance=Distance.COSINE)},
            sparse_vectors_config={"sparse": SparseVectorParams()}
        )


def add_documents_to_qdrant(documents):
    client = get_qdrant_client()
    dense_emb = get_dense_embeddings()
    sparse_emb = get_sparse_embeddings()
    vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME, embedding=dense_emb,
                                     vector_name="content", sparse_embedding=sparse_emb, sparse_vector_name="sparse",
                                     retrieval_mode=RetrievalMode.HYBRID)
    ids = [str(uuid4()) for _ in documents]
    vector_store.add_documents(documents=documents, ids=ids)


def delete_by_source(source_name):
    client = get_qdrant_client()
    if client.collection_exists(COLLECTION_NAME):
        client.delete(collection_name=COLLECTION_NAME, points_selector=Filter(
            must=[FieldCondition(key="metadata.source", match=MatchValue(value=source_name))]))


# ==============================================================================
# ARAYÃœZ
# ==============================================================================
with st.sidebar:
    try:
        st.image("bilimp_logo.png", width="stretch")
    except:
        st.warning("Logo Yok")

    st.markdown("### ğŸ› ï¸ Sistem AyarlarÄ±")
    if "last_role" not in st.session_state: st.session_state.last_role = "admin"
    current_user_role = st.selectbox("ğŸ‘¤ KullanÄ±cÄ± RolÃ¼", ["public", "user", "management", "admin", "private"], index=3)
    if current_user_role != st.session_state.last_role:
        st.session_state.messages = []
        st.session_state.last_role = current_user_role
        st.rerun()
    with st.expander("â„¹ï¸ Yetki DetayÄ±"):
        st.code(get_allowed_permissions(current_user_role))

    st.divider()
    st.markdown("### ğŸ§  Yapay Zeka Motoru")
    gemini_models_map = {"Gemini 2.5 Flash (HÄ±zlÄ±)": "gemini-2.5-flash",
                         "Gemini 3.0 Flash (AkÄ±llÄ± + HÄ±zlÄ±)": "gemini-3-flash-preview"}
    ollama_list = get_local_ollama_models()
    model_options = list(gemini_models_map.keys())
    if ollama_list:
        model_options.extend([f"Ollama: {m}" for m in ollama_list])
    else:
        model_options.append("Ollama (Model Yok)")

    selected_option = st.selectbox("Model SeÃ§imi", model_options)
    llm_model_id, llm_type = None, "ollama"
    if "Gemini" in selected_option:
        llm_type = "gemini";
        llm_model_id = gemini_models_map[selected_option]
    elif "Ollama" in selected_option:
        llm_type = "ollama";
        llm_model_id = selected_option.split(": ")[1]

    api_key = ""
    if llm_type == "gemini": api_key = st.text_input("ğŸ”‘ Google API Key", type="password")

    st.divider()
    st.markdown("### ğŸ›ï¸ Ä°nce Ayarlar")
    temperature = st.slider("YaratÄ±cÄ±lÄ±k", 0.0, 1.0, 0.1, step=0.1)
    top_k = st.number_input("BaÄŸlam (Chunk)", 1, 20, 5)
    score_threshold = st.slider("Benzerlik EÅŸiÄŸi", 0.0, 0.9, 0.70, step=0.05)
    with st.expander("ğŸ“„ Chunk Parametreleri"):
        c_size = st.number_input("Boyut", 500, 5000, 2500)
        c_over = st.number_input("Ã–rtÃ¼ÅŸme", 0, 1000, 200)

st.header("ğŸ“„ Bilimp DokÃ¼man AsistanÄ± (Streaming Agent)")
t1, t2 = st.tabs(["ğŸ“‚ **Belge YÃ¶netimi**", "ğŸ’¬ **AkÄ±llÄ± Sohbet**"])

# --- TAB 1: BELGE YÃ–NETÄ°MÄ° ---
with t1:
    col_upload, col_list = st.columns([1, 1], gap="large")
    with col_upload:
        st.markdown("#### â¬†ï¸ Belge YÃ¼kle")
        up_file = st.file_uploader("DosyayÄ± buraya sÃ¼rÃ¼kleyin", type=["pdf", "docx", "xlsx", "pptx"],
                                   label_visibility="collapsed")
        if up_file:
            bytes_data = up_file.getvalue();
            f_name = up_file.name;
            curr_md5 = calculate_md5(bytes_data);
            reg = load_registry()
            file_exists = False
            if f_name in reg:
                stored = reg[f_name]
                if isinstance(stored, dict) and stored["hash"] == curr_md5:
                    file_exists = True
                elif stored == curr_md5:
                    file_exists = True

            if file_exists:
                st.warning(f"âš ï¸ **{f_name}** zaten mevcut.")
            else:
                st.success(f"âœ… **{f_name}** analize hazÄ±r.")

            if st.button("ğŸš€ Sisteme Entegre Et", type="primary"):
                with st.status("Ä°ÅŸleniyor...", expanded=True) as s:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(f_name)[1]) as tmp:
                        tmp.write(bytes_data);
                        tmp_path = tmp.name
                    init_collection();
                    delete_by_source(f_name)
                    chunks = []
                    if f_name.endswith(".pptx"):
                        chunks = process_pptx_native(tmp_path, f_name, current_user_role)
                    else:
                        chunks = process_text_based(tmp_path, f_name, c_size, c_over, current_user_role)
                    if chunks:
                        add_documents_to_qdrant(chunks)
                        reg[f_name] = {"hash": curr_md5, "permission": current_user_role}
                        save_registry(reg)
                        s.update(label="TamamlandÄ±!", state="complete", expanded=False)
                        st.toast("BaÅŸarÄ±lÄ±!", icon="ğŸ‰");
                        time.sleep(1);
                        st.rerun()
                    else:
                        s.update(label="Hata", state="error");
                        st.error("AyrÄ±ÅŸtÄ±rÄ±lamadÄ±.")
                    os.unlink(tmp_path)

    with col_list:
        st.markdown("#### ğŸ—‚ï¸ Sistemdeki Belgeler")
        current_reg = load_registry();
        allowed_view_perms = get_allowed_permissions(current_user_role);
        visible_files = []
        for fname, fdata in current_reg.items():
            perm = fdata.get("permission", "public") if isinstance(fdata, dict) else "public"
            if perm in allowed_view_perms: visible_files.append((fname, perm))
        if not visible_files:
            st.info("GÃ¶rÃ¼ntÃ¼lenecek belge yok.")
        else:
            for fname, perm in visible_files:
                c1, c2 = st.columns([0.8, 0.2])
                with c1:
                    st.markdown(
                        f"""<div style="padding:10px; background:#161b22; border-radius:8px; margin-bottom:5px; border:1px solid #30363d;"><span style="color:white; font-weight:600;">ğŸ“„ {fname}</span><span style="background:#238636; color:white; padding:2px 8px; border-radius:4px; font-size:0.8em; margin-left:10px;">{perm}</span></div>""",
                        unsafe_allow_html=True)
                with c2:
                    if st.button("ğŸ—‘ï¸", key=f"del_{fname}"): delete_document_globally(fname); st.rerun()

# --- TAB 2: SOHBET (STREAMING) ---
# --- TAB 2: SOHBET (STREAMING) ---
with t2:
    # Memory iÃ§in yardÄ±mcÄ± fonksiyon
    def get_formatted_history(messages, max_pairs=5):
        """
        Mesaj geÃ§miÅŸini LangChain formatÄ±na Ã§evirir.
        max_pairs: Maksimum user-assistant Ã§ifti sayÄ±sÄ±
        """
        history = []
        # TÃ¼m mesajlarÄ± al
        all_msgs = messages.copy()

        # Son N Ã§ifti almak iÃ§in (her Ã§ift = 1 user + 1 assistant)
        # En fazla max_pairs * 2 mesaj al
        recent = all_msgs[-(max_pairs * 2):]

        for msg in recent:
            content = msg.get("content", "")
            if not content or content.strip() == "":
                continue

            if msg["role"] == "user":
                history.append(HumanMessage(content=content))
            elif msg["role"] == "assistant":
                history.append(AIMessage(content=content))

        return history


    # Mesaj geÃ§miÅŸini baÅŸlat
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # GeÃ§miÅŸ mesajlarÄ± render et
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])
            if m["role"] == "assistant" and "sources" in m and m["sources"]:
                with st.expander(f"ğŸ” Referans Kaynaklar ({len(m['sources'])})"):
                    for i, doc in enumerate(m['sources']):
                        score_val = doc.metadata.get("score", 0.0)
                        st.markdown(f"**#{i + 1}** | ğŸ“‚ `{doc.metadata.get('source')}` | ğŸ“Š Skor: `{score_val:.4f}`")
                        st.caption(doc.page_content)
                        st.divider()

    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
        # KullanÄ±cÄ± mesajÄ±nÄ± Ã–NCE ekle
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            client = get_qdrant_client()
            if not client.collection_exists(COLLECTION_NAME):
                st.error("VeritabanÄ± boÅŸ.")
            else:
                ready = True
                llm = None
                if llm_type == "gemini":
                    if not api_key:
                        st.error("API Key Eksik!")
                        ready = False
                    else:
                        llm = ChatGoogleGenerativeAI(
                            model=llm_model_id,
                            google_api_key=api_key,
                            temperature=temperature
                        )
                elif llm_type == "ollama":
                    if "Yok" in selected_option:
                        st.error("Model Yok!")
                        ready = False
                    else:
                        llm = ChatOllama(model=llm_model_id, temperature=temperature)

                if ready and llm:
                    try:
                        @tool
                        def bilimp_knowledge_base(query: str):
                            """
                            Bilimp AI AsistanÄ±'nÄ±n ÅŸirket iÃ§i bilgi bankasÄ±nda arama yapmasÄ±nÄ± saÄŸlar.
                            """
                            try:
                                dense_emb = get_dense_embeddings()
                                sparse_emb = get_sparse_embeddings()
                                vector_store = QdrantVectorStore(
                                    client=client,
                                    collection_name=COLLECTION_NAME,
                                    embedding=dense_emb,
                                    vector_name="content",
                                    sparse_embedding=sparse_emb,
                                    sparse_vector_name="sparse",
                                    retrieval_mode=RetrievalMode.HYBRID
                                )
                                
                                allowed_perms = get_allowed_permissions(current_user_role)
                                perm_filter = rest_models.Filter(must=[
                                    rest_models.FieldCondition(
                                        key="metadata.permission",
                                        match=rest_models.MatchAny(any=allowed_perms)
                                    )
                                ])
                                
                                results = vector_store.similarity_search_with_score(
                                    query, k=5, filter=perm_filter
                                )
                                
                                high_quality_docs = [
                                    doc for doc, score in results 
                                    if score >= 0.70
                                ]
                                
                                if not high_quality_docs:
                                    return "ARAÅTIRMA_SONUCU: Bu konuda belgelerimde yeterli kalitede bilgi bulunamadÄ±."
                                
                                if len(high_quality_docs) < 2:
                                    return "ARAÅTIRMA_SONUCU: Konu hakkÄ±nda Ã§ok az bilgi var, lÃ¼tfen daha spesifik soru sorun."
                                
                                context = "\n\n".join([doc.page_content for doc in high_quality_docs])
                                return f"ARAÅTIRMA_SONUCU: {context}"
                                
                            except Exception as e:
                                return f"ARAÅTIRMA_SONUCU: Teknik hata - {str(e)}"


                        llm_with_tools = llm.bind_tools([bilimp_knowledge_base])

                        # ---------------------------------------------------------
                        # 2. GEÃ‡MÄ°ÅÄ° DÃœZGÃœN FORMATTA HAZIRLA (KRÄ°TÄ°K DEÄÄ°ÅÄ°KLÄ°K)
                        # ---------------------------------------------------------
                        # SON mesajÄ± (ÅŸu anki prompt) HARÄ°Ã‡ tutarak geÃ§miÅŸi al
                        # Ã‡Ã¼nkÃ¼ prompt zaten ayrÄ±ca ekleniyor
                        history_messages = st.session_state.messages[:-1]  # Son mesaj hariÃ§
                        history_langchain_format = get_formatted_history(history_messages, max_pairs=5)

                        # ---------------------------------------------------------
                        # 3. SÄ°STEM PROMPTU
                        # ---------------------------------------------------------
                        identity_section = """
                                Sen profesyonel, yardÄ±msever ve kurumsal bir asistansÄ±n.
                                KÄ°MLÄ°ÄÄ°N:
                                - AdÄ±n: **Bilimp AI AsistanÄ±**.
                                - GÃ¶revin: Ã‡alÄ±ÅŸanlara ÅŸirket iÃ§i dÃ¶kÃ¼manlar, yÃ¶netmelikler ve prosedÃ¼rler hakkÄ±nda bilgi saÄŸlamak.
                                
                                YETENEKLERÄ°N VE HAFIZA:
                                - GÃ¼Ã§lÃ¼ bir hafÄ±zan var. Sohbet geÃ§miÅŸindeki TÃœM mesajlarÄ± (hem kullanÄ±cÄ±nÄ±n sorularÄ±nÄ± HEM DE kendi verdiÄŸin cevaplarÄ±) hatÄ±rlarsÄ±n.
                                - KullanÄ±cÄ± "Ã–nceki soruma ne cevap verdin?", "Az Ã¶nce ne dedin?", "Bir Ã¶nceki cevabÄ±n neydi?" gibi sorular sorarsa, sohbet geÃ§miÅŸine bakarak KENDÄ° VERDÄ°ÄÄ°N CEVAPLARI sÃ¶yle.
                                - KullanÄ±cÄ± "Ne sormuÅŸtum?" derse, onun Ã¶nceki sorularÄ±nÄ± hatÄ±rla.
                                
                                DAVRANIÅ KURALLARI:
                                1. EÄŸer kullanÄ±cÄ± "Kimsin?" derse kendini tanÄ±t.
                                2. BaÅŸka bir model olduÄŸunu ASLA SÃ–YLEME.
                                3. KullanÄ±cÄ±ya her zaman nazik ve "siz" diliyle hitap et.
                                4. HafÄ±za sorularÄ± iÃ§in TOOL KULLANMA, direkt sohbet geÃ§miÅŸinden cevapla.
                                """

                        router_section = """
                                GÃ–REVÄ°N:
                                Gelen soruyu ve sohbet geÃ§miÅŸini analiz edip 'bilimp_knowledge_base' aracÄ±nÄ± kullanÄ±p kullanmayacaÄŸÄ±na karar ver.
                                
                                KARAR MANTIÄI:
                                1. **Veri Ä°steÄŸi:** Åirket verisi, sayÄ±, kural soruluyorsa -> TOOL KULLAN.
                                2. **Takip Sorusu:** "Peki kaÃ§ tane?", "Bunun fiyatÄ± ne?" gibi Ã¶nceki konunun devamÄ±ysa -> TOOL KULLAN.
                                3. **HAFIZA SORULARI (KRÄ°TÄ°K):**
                                   - "Ã–nceki cevabÄ±n neydi?", "Ne demiÅŸtin?", "Az Ã¶nce ne sÃ¶yledin?" -> TOOL KULLANMA, sohbet geÃ§miÅŸinden cevapla.
                                   - "Ne sormuÅŸtum?", "Ã–nceki sorum neydi?" -> TOOL KULLANMA, sohbet geÃ§miÅŸinden cevapla.
                                4. **Sohbet:** "Merhaba", "NasÄ±lsÄ±n" -> TOOL KULLANMA.
                                """

                        full_system_prompt = identity_section + "\n\n" + router_section

                        # ---------------------------------------------------------
                        # 4. MESAJLARI OLUÅTUR
                        # ---------------------------------------------------------
                        input_msgs = [
                                         SystemMessage(content=full_system_prompt)
                                     ] + history_langchain_format + [
                                         HumanMessage(content=prompt)
                                     ]

                        # DEBUG: GeÃ§miÅŸi kontrol et (geliÅŸtirme aÅŸamasÄ±nda kullan)
                        # st.write(f"ğŸ“œ GeÃ§miÅŸ mesaj sayÄ±sÄ±: {len(history_langchain_format)}")
                        # for i, msg in enumerate(history_langchain_format):
                        #     st.write(f"{i}: {type(msg).__name__} - {msg.content[:50]}...")

                        # Model Karar Veriyor
                        ai_msg = llm_with_tools.invoke(input_msgs)

                        # DeÄŸiÅŸkenleri sÄ±fÄ±rla
                        final_response = ""
                        retrieved_docs = []

                        # ---------------------------------------------------------
                        # 5. DURUMA GÃ–RE CEVAPLAMA
                        # ---------------------------------------------------------
                        if ai_msg.tool_calls:
                            # DURUM A: RAG GEREKLÄ°
                            with st.status("ğŸ“š Bilgi BankasÄ± TaranÄ±yor...", expanded=True) as s:
                                dense_emb = get_dense_embeddings()
                                sparse_emb = get_sparse_embeddings()
                                vector_store = QdrantVectorStore(
                                    client=client,
                                    collection_name=COLLECTION_NAME,
                                    embedding=dense_emb,
                                    vector_name="content",
                                    sparse_embedding=sparse_emb,
                                    sparse_vector_name="sparse",
                                    retrieval_mode=RetrievalMode.HYBRID
                                )
                                allowed_perms = get_allowed_permissions(current_user_role)
                                perm_filter = rest_models.Filter(must=[
                                    rest_models.FieldCondition(
                                        key="metadata.permission",
                                        match=rest_models.MatchAny(any=allowed_perms)
                                    )
                                ])

                                results = vector_store.similarity_search_with_score(prompt, k=top_k, filter=perm_filter)
                                for doc, score in results:
                                    if score >= score_threshold:
                                        doc.metadata["score"] = score
                                        retrieved_docs.append(doc)

                                if not retrieved_docs:
                                    s.update(label="Bilgi BulunamadÄ±", state="error", expanded=False)
                                    final_response = "Bu konuda belgelerimde yeterli kalitede bilgi bulunamadÄ±. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde ifade edin."
                                    st.error("âŒ " + final_response)
                                    st.session_state.messages.append({
                                        "role": "assistant",
                                        "content": final_response,
                                        "sources": []
                                    })
                                    st.stop()
                                
                                if len(retrieved_docs) < 2:
                                    s.update(label="Yetersiz Bilgi", state="warning", expanded=False)
                                    final_response = "Bu konuda Ã§ok az bilgi var. LÃ¼tfen daha spesifik bir soru sorun."
                                    st.warning("âš ï¸ " + final_response)
                                    st.session_state.messages.append({
                                        "role": "assistant",
                                        "content": final_response,
                                        "sources": retrieved_docs
                                    })
                                    st.stop()

                                context_str = "\n\n".join([d.page_content for d in retrieved_docs])
                                s.update(label="Bilgiler Getirildi!", state="complete", expanded=False)

                            rag_system_prompt = f"""
SÄ°STEM TALÄ°MATI: Sen TÃœBÄ°TAK 1505 dokÃ¼man uzmanÄ±sÄ±n.

BULUNAN DÃ–KÃœMANLAR:
{context_str}

KATÃ KURALLAR:
1. SADECE yukarÄ±daki belgelerden cevap ver.
2. Belgeler soruyu tam cevaplamÄ±yorsa bunu aÃ§Ä±kÃ§a belirt.
3. Belirsizlik varsa "Mevcut belgeler bu konuda net bilgi iÃ§ermiyor" de.
4. ASLA tahmin yapma, spekÃ¼lasyon etme veya kendi bilgini ekleme.
5. Her cevabÄ±n sonunda hangi belgeden aldÄ±ÄŸÄ±nÄ± belirt.
6. CevabÄ±n TAMAMEN TÃ¼rkÃ§e olmalÄ±dÄ±r.

UYARI: YukarÄ±daki belgeler soruyu cevaplamak iÃ§in yetersizse bunu kullanÄ±cÄ±ya sÃ¶yle.
"""
                            st.markdown("ğŸ“š **DÃ¶kÃ¼manlardan YanÄ±tlanÄ±yor:**")

                            # RAG iÃ§in de geÃ§miÅŸi ekle
                            rag_messages = [
                                               SystemMessage(content=rag_system_prompt)
                                           ] + history_langchain_format + [
                                               HumanMessage(content=prompt)
                                           ]

                            stream_generator = llm.stream(rag_messages)
                            final_response = st.write_stream(stream_generator)
                            
                            is_valid, validation_msg = HallucinationValidator.validate_response(
                                prompt, final_response, retrieved_docs
                            )
                            
                            if not is_valid:
                                st.warning(f"âš ï¸ Kalite UyarÄ±sÄ±: {validation_msg}")
                                final_response = "Bu konuda belgelerimde net bilgi bulamadÄ±m. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde ifade edin."
                                st.error(final_response)

                        else:
                            # DURUM B: SOHBET (Tool Yok)
                            raw_content = ai_msg.content
                            content_text = ""

                            if isinstance(raw_content, str):
                                content_text = raw_content
                            elif isinstance(raw_content, list):
                                for item in raw_content:
                                    if isinstance(item, list):
                                        for sub_item in item:
                                            if isinstance(sub_item, dict):
                                                content_text += sub_item.get("text", "")
                                    elif isinstance(item, dict):
                                        content_text += item.get("text", "")
                                    elif isinstance(item, str):
                                        content_text += item
                            else:
                                content_text = str(raw_content)

                            st.markdown("ğŸ’¬ **Sohbet Modu:**")
                            final_response = st.write_stream(stream_text_generator(content_text))

                        # ---------------------------------------------------------
                        # 6. GEÃ‡MÄ°ÅE KAYDET (KRÄ°TÄ°K!)
                        # ---------------------------------------------------------
                        if retrieved_docs:
                            with st.expander(f"ğŸ” Referans Kaynaklar ({len(retrieved_docs)})"):
                                for i, doc in enumerate(retrieved_docs):
                                    score_val = doc.metadata.get("score", 0.0)
                                    st.markdown(
                                        f"**#{i + 1}** | ğŸ“‚ `{doc.metadata.get('source')}` | ğŸ“Š Skor: `{score_val:.4f}`")
                                    st.caption(doc.page_content)

                        # Assistant cevabÄ±nÄ± kaydet
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": final_response,  # Tam cevap metni
                            "sources": retrieved_docs
                        })

                    except Exception as e:
                        error_msg = str(e)
                        if "429" in error_msg:
                            st.error("âš ï¸ API KotasÄ± Doldu.")
                        else:
                            st.error(f"Hata: {e}")
                        # Hata durumunda da boÅŸ mesaj ekleme
                        st.session_state.messages.append({
                            "role": "assistant",
                            "content": f"Bir hata oluÅŸtu: {error_msg}",
                            "sources": []
                        })